{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "train = pd.read_csv('train.csv')\n",
    "population_demo = pd.read_csv('auxiliary-data/sg-population-demographics.csv')\n",
    "\n",
    "# all the auxiliary variables done by Fiona\n",
    "station = pd.read_csv('train_df_fe_station.csv') \n",
    "malls = pd.read_csv('train_df_fe_malls.csv')\n",
    "hawker = pd.read_csv('train_df_fe_hawker.csv')\n",
    "commercial = pd.read_csv('train_df_fe_commercial.csv')\n",
    "# auxilliary variables chosen\n",
    "aux = [station, malls, hawker, commercial]\n",
    "aux_df = pd.concat(aux, axis = 1) # df of all auxilliary variables\n",
    "aux_chosen = aux_df[['commercial_CBD', 'commercial_type_CR',\n",
    "       'commercial_type_IEBP', 'commercial_type_IEPB', 'commercial_type_BN',\n",
    "       'commercial_type_IHL', 'hawker_ECLFV', 'hawker_NFC', 'hawker_CRB89',\n",
    "       'hawker_OARB51OARFCSM', 'hawker_CRB', 'hawker_HVMFC', 'hawker_BFC',\n",
    "       'hawker_CCFC', 'hawker_TBM', 'hawker_BPHC', 'hawker_GMFC',\n",
    "       'hawker_YPHC', 'hawker_OTH', 'hawker_KAHC', 'hawker__',\n",
    "       'hawker_highrating_', 'hawker_established_', 'malls_GWC', 'malls_IO',\n",
    "       'malls_TSMBS', 'malls_NAC', 'malls_PS', 'malls_SC', 'malls_OTH',\n",
    "       'malls_CA', 'malls_JCA', 'malls_VivoCity', 'malls_JP', 'malls__',\n",
    "       'malls_ratingsbin_4.1', 'malls_ratingsbin_4.3', 'malls_ratingsbin_>4.0',\n",
    "       'malls_ratingsbin_4.2', 'malls_ratingsbin_4.0',\n",
    "       'malls_ratingsbin_>=4.5', 'malls_ratingsbin_4.4', 'malls_established_',\n",
    "       'station_type_mrt', 'station_type_other', 'station_interchange_',\n",
    "       'station_EW_', 'station_NS_', 'station_NE_', 'station_CC_',\n",
    "       'station_DT_']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['flat_type'] == \"1-room\", 'flat_type'] = \"1 room\"\n",
    "train.loc[train['flat_type'] == \"2-room\", 'flat_type'] = \"2 room\"\n",
    "train.loc[train['flat_type'] == \"3-room\", 'flat_type'] = \"3 room\"\n",
    "train.loc[train['flat_type'] == \"4-room\", 'flat_type'] = \"4 room\"\n",
    "train.loc[train['flat_type'] == \"5-room\", 'flat_type'] = \"5 room\"\n",
    "\n",
    "# converting the block column to 1 if it has the number 4\n",
    "# converting the block column to 0 if it does not have the number 4\n",
    "train.loc[train['block'].str.contains('4'),'block'] = 1\n",
    "train.loc[train['block'].str.contains('4') == False, 'block'] = 0\n",
    "\n",
    "# convert to 01 to 06, 06 to 10, 10 to 15, 16 to 21, 21 to 25, 25 to 30, \n",
    "# 31 to 36, 36 to 40, 40 to 45, 46 to 51\n",
    "# data is messy as it has lots of overlaps, so the partioning is to make\n",
    "# it more systematic\n",
    "# 01 to 06\n",
    "train.loc[train['storey_range'] == \"01 to 03\", 'storey_range'] = \"01 to 06\"\n",
    "train.loc[train['storey_range'] == \"01 to 05\", 'storey_range'] = \"01 to 06\"\n",
    "train.loc[train['storey_range'] == \"04 to 06\", 'storey_range'] = \"01 to 06\"\n",
    "# 06 to 10\n",
    "train.loc[train['storey_range'] == \"07 to 09\", 'storey_range'] = \"06 to 10\"\n",
    "# 10 to 15\n",
    "train.loc[train['storey_range'] == \"10 to 12\", 'storey_range'] = \"10 to 15\"\n",
    "train.loc[train['storey_range'] == \"11 to 15\", 'storey_range'] = \"10 to 15\"\n",
    "train.loc[train['storey_range'] == \"13 to 15\", 'storey_range'] = \"10 to 15\"\n",
    "# 16 to 21\n",
    "train.loc[train['storey_range'] == \"16 to 18\", 'storey_range'] = \"16 to 21\"\n",
    "train.loc[train['storey_range'] == \"16 to 20\", 'storey_range'] = \"16 to 21\"\n",
    "train.loc[train['storey_range'] == \"19 to 21\", 'storey_range'] = \"16 to 21\"\n",
    "# 21 to 25\n",
    "train.loc[train['storey_range'] == \"22 to 24\", 'storey_range'] = \"21 to 25\"\n",
    "# 25 to 30\n",
    "train.loc[train['storey_range'] == \"25 to 27\", 'storey_range'] = \"25 to 30\"\n",
    "train.loc[train['storey_range'] == \"26 to 30\", 'storey_range'] = \"25 to 30\"\n",
    "train.loc[train['storey_range'] == \"28 to 30\", 'storey_range'] = \"25 to 30\"\n",
    "# 31 to 36\n",
    "train.loc[train['storey_range'] == \"31 to 33\", 'storey_range'] = \"31 to 36\"\n",
    "train.loc[train['storey_range'] == \"31 to 35\", 'storey_range'] = \"31 to 36\"\n",
    "train.loc[train['storey_range'] == \"34 to 36\", 'storey_range'] = \"31 to 36\"\n",
    "# 36 to 40\n",
    "train.loc[train['storey_range'] == \"37 to 39\", 'storey_range'] = \"36 to 40\"\n",
    "# 40 to 45\n",
    "train.loc[train['storey_range'] == \"40 to 42\", 'storey_range'] = \"40 to 45\"\n",
    "train.loc[train['storey_range'] == \"43 to 45\", 'storey_range'] = \"40 to 45\"\n",
    "# 46 to 51\n",
    "train.loc[train['storey_range'] == \"46 to 48\", 'storey_range'] = \"46 to 51\"\n",
    "train.loc[train['storey_range'] == \"49 to 51\", 'storey_range'] = \"46 to 51\"\n",
    "\n",
    "# population count across age in a particular subzone\n",
    "dicts = {}\n",
    "for area in np.unique(population_demo.subzone):\n",
    "    area_count = population_demo[population_demo['subzone'] == area]['count'].sum()\n",
    "    dicts[area] = area_count \n",
    "train['popcount_subzone'] = train['subzone'].map(dicts)\n",
    "\n",
    "# 490 was derived from central subzone in the population demographics\n",
    "# dataset. However, there is no such subzone in the main dataset. After\n",
    "# verifying it, central subzone is inferred to be 'city hall' in main \n",
    "# data set (beach road area)\n",
    "print(dicts['central subzone'])\n",
    "train.loc[train['subzone'] == \"city hall\", 'popcount_subzone'] = 490\n",
    "\n",
    "train[['resale_year', 'resale_month']] = train['month'].str.split('-', 1, expand=True)\n",
    "\n",
    "\n",
    "train['age_at_sales'] = pd.to_numeric(train['resale_year']) - pd.to_numeric(train['lease_commence_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement one-hot encoding on categorical columns\n",
    "# do note that pd.get_dummies drop the original variable column by\n",
    "# default. Also note that \"block\" does not have to be one-hot encoded\n",
    "# because it is a binary variable .\n",
    "categorical_cols = ['flat_type', 'street_name','resale_year','resale_month',\n",
    "                    'storey_range', 'flat_model', 'subzone', \n",
    "                    'planning_area', 'region', 'lease_commence_date']\n",
    "train_dummies = pd.get_dummies(train, columns = categorical_cols)\n",
    "\n",
    "train_y_all = train_dummies['resale_price']\n",
    "\n",
    "\n",
    "train_final_all = train_dummies.drop(columns = ['town', 'eco_category', \n",
    "                                            'elevation','month','resale_price'])\n",
    "\n",
    "combined = [train_final_all, aux_chosen]\n",
    "train_final_all = pd.concat(combined, axis = 1)\n",
    "\n",
    "# splitting to training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_final_all, train_y_all, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Normalize data using the sciki-learn MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train, X_val = scaler.transform(X_train), scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'base_estimator__max_depth': [3,20,50],\n",
    "    'base_estimator__min_samples_leaf': [2,20,50],\n",
    "    'n_estimators': [1500],\n",
    "    'learning_rate':[0.1,0.01,0.001],\n",
    "    'loss': ['linear','square']\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "adaboost = AdaBoostRegressor(base_estimator=DecisionTreeRegressor())\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_ab = GridSearchCV(estimator = adaboost, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, \n",
    "                           scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_ab.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Store the parameters of the best model\n",
    "best_params = grid_search_ab.best_params_\n",
    "\n",
    "# Predict class labels of test data on the model with the best found parameters\n",
    "y_pred = grid_search_ab.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE score\n",
    "best_RMSE = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "print('Adaboost regressor: {} (RMSE: {:.3f})'.format(best_params, best_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [70,150,None],\n",
    "    'max_features': ['sqrt',100,500],# might change this to a much smaller value\n",
    "    'min_samples_split': [2,20,40],\n",
    "    'min_samples_leaf': [1,20,40],\n",
    "    'n_estimators': [1500]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, \n",
    "                           scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Store the parameters of the best model\n",
    "best_params = grid_search_rf.best_params_\n",
    "\n",
    "# Predict class labels of test data on the model with the best found parameters\n",
    "y_pred = grid_search_rf.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE score\n",
    "best_RMSE = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "print('randomforest regressor: {} (RMSE: {:.3f})'.format(best_params, best_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50,50),(100,100,100,100), (200,200,200,200),\n",
    "                          (50,50,50,50,50,50),(100,100,100,100,100,100),(200,200,200,200,200,200)],\n",
    "    'alpha': [0.00005,0.0005, 0.005],\n",
    "    'activation':['relu'],\n",
    "    'solver':['adam'],\n",
    "    'max_iter':[1000],\n",
    "    'learning_rate':['adaptive']\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "mlp = MLPRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_mlp = GridSearchCV(estimator = mlp, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, \n",
    "                           scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Store the parameters of the best model\n",
    "best_params = grid_search_mlp.best_params_\n",
    "\n",
    "# Predict class labels of test data on the model with the best found parameters\n",
    "y_pred = grid_search_mlp.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE score\n",
    "best_RMSE = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "print('MLP regressor: {} (RMSE: {:.3f})'.format(best_params, best_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [5,15,45,100,200]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_knn = GridSearchCV(estimator = knn, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, \n",
    "                           scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Store the parameters of the best model\n",
    "best_params = grid_search_knn.best_params_\n",
    "\n",
    "# Predict class labels of test data on the model with the best found parameters\n",
    "y_pred = grid_search_knn.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE score\n",
    "best_RMSE = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "print('KNN regressor: {} (RMSE: {:.3f})'.format(best_params, best_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# grid search \n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'C': [0.1,0.5,1,10],\n",
    "    'degree': [3,6],\n",
    "    'kernel': ['rbf','poly'],\n",
    "    'gamma': ['scale'],\n",
    "    'max_iter':[-1]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "svm = SVR()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_svm = GridSearchCV(estimator = svm, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2,\n",
    "                          scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Store the parameters of the best model\n",
    "best_params = grid_search_svm.best_params_\n",
    "\n",
    "# Predict class labels of test data on the model with the best found parameters\n",
    "y_pred = grid_search_svm.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE score\n",
    "best_RMSE = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "print('SVM regressor: {} (RMSE: {:.3f})'.format(best_params, best_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 20,40],\n",
    "    'min_samples_split': [2,20,40]\n",
    "    'min_samples_leaf': [1,20,40],\n",
    "    'n_estimators': [1500],\n",
    "    'learning_rate':[0.1]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_gbr = GridSearchCV(estimator = gbr, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2, \n",
    "                           scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_gbr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Store the parameters of the best model\n",
    "best_params = grid_search_gbr.best_params_\n",
    "\n",
    "# Predict class labels of test data on the model with the best found parameters\n",
    "y_pred = grid_search_gbr.predict(X_val)\n",
    "\n",
    "# Calculate the RMSE score\n",
    "best_RMSE = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "print('GB regressor: {} (RMSE: {:.3f})'.format(best_params, best_RMSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
